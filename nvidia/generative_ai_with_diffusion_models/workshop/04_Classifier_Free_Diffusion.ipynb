{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2-Px6LAIoz7"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classifier-Free Diffusion Guidance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have been able to train a model to generate images of clothing using the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. However, there is no way for the user to specify what kind of images should be generated. Let's fix that by using [Classifier-Free Diffusion Guidance](https://arxiv.org/pdf/2207.12598.pdf), a relatively simple way to create a [Conditional Diffusion Model](https://github.com/TeaPearce/Conditional_Diffusion_MNIST/tree/main).\n",
    "\n",
    "#### Learning Objectives\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Add categorical embeddings to a U-Net\n",
    "* Train a model with a Bernoulli mask\n",
    "* Add a weight the reverse diffusion process\n",
    "* Practice learnings on a more challenging dataset\n",
    "\n",
    "\n",
    "Before we get started, let's load the necessary libraries and dataset information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWn2WgPaIoz8"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "# User defined libraries\n",
    "from utils import other_utils\n",
    "from utils import ddpm_utils\n",
    "from utils import UNet_utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 16\n",
    "IMG_CH = 1\n",
    "BATCH_SIZE = 128\n",
    "N_CLASSES = 10\n",
    "data, dataloader = other_utils.load_transformed_fashionMNIST(IMG_SIZE, BATCH_SIZE)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `B`eta schedule from before will stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 10\n",
    "ncols = 15\n",
    "\n",
    "T = nrows * ncols\n",
    "B_start = 0.0001\n",
    "B_end = 0.02\n",
    "B = torch.linspace(B_start, B_end, T).to(device)\n",
    "ddpm = ddpm_utils.DDPM(B, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, our `UNet` is slightly different. We've added a few changes and moved the U-Net architecture into its own [UNet_utils.py](utils/UNet_utils.py) folder.\n",
    "\n",
    "In the `__init__` function, we've added a new parameter: `c_embed_dim`. Like for timestep `t`, we can create an embedding for our class categories.\n",
    "\n",
    "```python\n",
    "        self.sinusoidaltime = SinusoidalPositionEmbedBlock(t_embed_dim)\n",
    "        self.t_emb1 = EmbedBlock(t_embed_dim, up_chs[0])\n",
    "        self.t_emb2 = EmbedBlock(t_embed_dim, up_chs[1])\n",
    "        self.c_embed1 = EmbedBlock(c_embed_dim, up_chs[0])  # New\n",
    "        self.c_embed2 = EmbedBlock(c_embed_dim, up_chs[1])  # New\n",
    "```\n",
    "\n",
    "Next, in the `forward` function, we have two new parameters: `c` and `c_mask`.\n",
    "* `c` is a vector representing our categorical input. It can be a [one-hot encoding](https://www.kaggle.com/code/dansbecker/using-categorical-data-with-one-hot-encoding) or an embedding vector.\n",
    "* `c_mask` is used to randomly set values within `c` to zero. This helps the model learn what an average output might be without a categorical input, like in the previous notebook. \n",
    "\n",
    "```python\n",
    "        c = c * c_mask\n",
    "        c_emb1 = self.c_embed1(c)\n",
    "        c_emb2 = self.c_embed2(c)\n",
    "```\n",
    "\n",
    "There are many different ways we can combine this embedded categorical information into the model. One popular method is with [scaling and shifting](https://arxiv.org/pdf/2210.08823.pdf). We can scale (multiply) our categorical embedding to the latent image and then (add) our time embedding `t_emb`. The scale and shift act as a sort of variance and average respectively.\n",
    "\n",
    "```python\n",
    "        up0 = self.up0(latent_vec)\n",
    "        up1 = self.up1(c_emb1 * up0 + t_emb1, down2)  # Changed\n",
    "        up2 = self.up2(c_emb2 * up1 + t_emb2, down1)  # Changed\n",
    "        return self.out(torch.cat((up2, down0), 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert our label to a format that can be processed by the model using the `get_context_mask` function below. Since our label is a single integer, we can use [F.one_hot](https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html) to turn it into an encoding vector.\n",
    "\n",
    "To randomly set values from this one hot encoding to zero, we can use the [Bernoulli](https://mathworld.wolfram.com/BernoulliDistribution.html) distribution. This distribution is like flipping a weighted coin. \"Heads\" will land $p$ percent of the time and \"Tails\" will land $1-p$ percent of the time. In this case, our `drop_prob` represents \"Tails\".\n",
    "\n",
    "<center><img src=\"images/bernoulli.png\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_mask(c, drop_prob):\n",
    "    c_hot = F.one_hot(c.to(torch.int64), num_classes=N_CLASSES).to(device)\n",
    "    c_mask = torch.bernoulli(torch.ones_like(c_hot).float() - drop_prob).to(device)\n",
    "    return c_hot, c_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all the changes we need to add to our `UNet` for it to learn from categorical data. Let's go ahead and build an instance of this new structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet_utils.UNet(\n",
    "    T, IMG_CH, IMG_SIZE, down_chs=(64, 64, 128), t_embed_dim=8, c_embed_dim=N_CLASSES\n",
    ")\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "model = torch.compile(model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to tell what the model is trying to generate, let's keep track of the different class names. The order here matches the label order of the dataset. For example, when the label is 3, it represents a dress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"Top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our train step is almost the same as last time. We will cycle through each class at each preview step so we can see how the model learns across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 3\n",
    "preview_c = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        c_drop_prob = 0.1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).float()\n",
    "        x = batch[0].to(device)\n",
    "        c_hot, c_mask = get_context_mask(batch[1], c_drop_prob)  # New\n",
    "        loss = ddpm.get_loss(model, x, t, c_hot, c_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 1 == 0 and step % 100 == 0:\n",
    "            class_name = class_names[preview_c]\n",
    "            print(f\"Epoch {epoch} | Step {step:03d} | Loss: {loss.item()} | C: {class_name}\")\n",
    "            c_drop_prob = 0 # Do not drop context for preview\n",
    "            c_hot, c_mask = get_context_mask(torch.Tensor([preview_c]), c_drop_prob)\n",
    "            ddpm.sample_images(model, IMG_CH, IMG_SIZE, ncols, c_hot, c_mask)\n",
    "            preview_c = (preview_c + 1) % N_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did it do? Try running the code cell below to see the final result for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "ncols = 3\n",
    "c_drop_prob = 0 # Change me to a value between 1 and 0\n",
    "\n",
    "for c in range(10):\n",
    "    print(class_names[c])\n",
    "    c_hot, c_mask = get_context_mask(torch.Tensor([c]), c_drop_prob)\n",
    "    ddpm.sample_images(model, IMG_CH, IMG_SIZE, ncols, c_hot, c_mask, axis_on=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Conditioning Reverse Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but not great either. Some of the classes are still influencing each other. For example, this shoe seems to have a shirt sleeve hanging off it.\n",
    "\n",
    "<center><img src=\"images/shirt_shoe.png\" alt=\"shirt_shoe\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "We can fix this by increasing the \"weight\" of the category. Here's the strategy:\n",
    "* During the reverse diffusion process at each timestep, we will denoise the image **twice**\n",
    "  * The first noise image we extract will **keep** its classifier information (`e_t_keep_c`)\n",
    "  * The second noise image we extract will **drop** its classifier information (`e_t_drop_c`)\n",
    "* We will subtract the average noise from the categorical noise using:\n",
    "  * `e_t = (1 + w) * e_t_keep_c - w * e_t_drop_c`\n",
    "  * where `w` is a weight value we choose as a hyperparameter\n",
    "* We will use this new `e_t` noise to perform diffusion with `reverse_q`.\n",
    "* Repeat the above steps from `t` = `T` to `0`\n",
    "\n",
    "<center><img src=\"images/weighted_reverse_diffusion.png\"/></center>\n",
    "\n",
    "We've defined these steps in `sample_w` below. `sample_w` takes a list of noise weights so we can compare how it impacts the diffusion results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_w(\n",
    "    model, input_size, T, c, w_tests=[-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0], store_freq=10\n",
    "):\n",
    "    # Preprase \"grid of samples\" with w for rows and c for columns\n",
    "    n_samples = len(w_tests) * len(c)\n",
    "\n",
    "    # One w for each c\n",
    "    w = torch.tensor(w_tests).float().repeat_interleave(len(c))\n",
    "    w = w[:, None, None, None].to(device)  # Make w broadcastable\n",
    "    x_t = torch.randn(n_samples, *input_size).to(device)\n",
    "\n",
    "    # One c for each w\n",
    "    c = c.repeat(len(w_tests), 1)\n",
    "\n",
    "    # Double the batch\n",
    "    c = c.repeat(2, 1)\n",
    "\n",
    "    # Don't drop context at test time\n",
    "    c_mask = torch.ones_like(c).to(device)\n",
    "    c_mask[n_samples:] = 0.0\n",
    "\n",
    "    x_t_store = []\n",
    "    for i in range(0, T)[::-1]:\n",
    "        # Duplicate t for each sample\n",
    "        t = torch.tensor([i]).to(device)\n",
    "        t = t.repeat(n_samples, 1, 1, 1)\n",
    "\n",
    "        # Double the batch\n",
    "        x_t = x_t.repeat(2, 1, 1, 1)\n",
    "        t = t.repeat(2, 1, 1, 1)\n",
    "\n",
    "        # Find weighted noise\n",
    "        e_t = model(x_t, t, c, c_mask)\n",
    "        e_t_keep_c = e_t[:n_samples]\n",
    "        e_t_drop_c = e_t[n_samples:]\n",
    "        e_t = (1 + w) * e_t_keep_c - w * e_t_drop_c\n",
    "\n",
    "        # Deduplicate batch for reverse diffusion\n",
    "        x_t = x_t[:n_samples]\n",
    "        t = t[:n_samples]\n",
    "        x_t = ddpm.reverse_q(x_t, t, e_t)\n",
    "\n",
    "        # Store values for animation\n",
    "        if i % store_freq == 0 or i == T or i < 10:\n",
    "            x_t_store.append(x_t)\n",
    "\n",
    "    x_t_store = torch.stack(x_t_store)\n",
    "    return x_t, x_t_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to see it in action! Run the code below generate some articles of clothing and arrange them into a grid with [make_grid](https://pytorch.org/vision/main/generated/torchvision.utils.make_grid.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.arange(N_CLASSES).to(device)\n",
    "c_drop_prob = 0  # Keep all category information for sampling\n",
    "c_hot, c_mask = get_context_mask(c, c_drop_prob)\n",
    "input_size = (IMG_CH, IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "x_0, x_t_store = sample_w(model, input_size, T, c_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [other_utils.to_image(make_grid(x_t.cpu(), nrow=N_CLASSES)) for x_t in x_t_store]\n",
    "other_utils.save_animation(grids, \"04_images/fashionMNIST.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click [here](04_images/fashionMNIST.gif) to see the animation of the reverse diffusion process that was just generated.\n",
    "\n",
    "The rows represent increasing `w` values from `[-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0]`. The first two rows are negative, meaning it stresses the average value of the model more than the categorical value. The model will sometimes generate completely different articles of clothing than was intended. The last few rows are consistent in generating items that match their label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Modified TF Flowers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on getting this far! You've mastered FashionMNIST, so now it's time for a bigger challenge: color images. For this challenge, we'll be using a modified version of the [TF Flowers](https://www.tensorflow.org/datasets/catalog/tf_flowers) dataset.\n",
    "\n",
    "These images have been slightly modified for the purpose of image generation. For example, this photo by user \"_e.t\" has been cropped to focus on the flower.\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/24459750_eb49f6e4cb_m.jpg\";/>\n",
    "    <img src=\"data/cropped_flowers/sunflowers/24459750_eb49f6e4cb_m.jpg\";/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will take much longer to train because of this extra dimension of color. To speed things up, let's preload the images onto the GPU. If we [resize](https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html) them before loading them onto our GPU, they will not take up much space.\n",
    "\n",
    "We can use this technique because the dataset is relatively small. With a larger dataset, this may not be feasible to do.\n",
    "\n",
    "To start, we should define the dataset variables:\n",
    "* Image width and height `IMG_SIZE`\n",
    "* Number of image channels `IMG_CH`\n",
    "* Batch size `BATCH_SIZE`\n",
    "* The size of the generated image `INPUT_SIZE` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 32\n",
    "IMG_CH = 3\n",
    "BATCH_SIZE = 128\n",
    "INPUT_SIZE = (IMG_CH, IMG_SIZE, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to store the images on the GPU, we will have a list of `pre_transforms` that will run one time when we initialize our dataset. Then, the `random_transforms` will run on each batch when it is pulled from the dataset.\n",
    "\n",
    "[Resize](https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html) will resize an image so that the smaller edge between the width and height matches the size we specify. Then, we can use [RandomCrop](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomCrop.html) to both make the image square and effectively increase the size of our dataset with random data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "pre_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),  # Scales data into [0,1]\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1)  # Scale between [-1, 1]\n",
    "])\n",
    "\n",
    "random_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's develop a function to read in the image files. We'll use each image's parent directly to identify its label. We have three categories listed in `DATA_LABELS` below.\n",
    "\n",
    "<center>\n",
    "    <img src=\"data/cropped_flowers/daisy/14219214466_3ca6104eae_m.jpg\";/>\n",
    "    <img src=\"data/cropped_flowers/sunflowers/1240625276_fb3bd0c7b1.jpg\";/>\n",
    "    <img src=\"data/cropped_flowers/roses/14510185271_b5d75dd98e_n.jpg\";/>\n",
    "</center>\n",
    "\n",
    "From left to right, we have a `daisy` by Allison Brown, a `sunflower` by Rob Young, and a `rose` by Matteo Accattino. The author for each photo is listed in [LICENSE.txt](data/cropped_flowers/LICENSE.txt)\n",
    "\n",
    "Let's use the [glob](https://docs.python.org/3/library/glob.html) function to programmatically fetch the datapaths for each flower photo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/cropped_flowers/\"\n",
    "DATA_LABELS = [\"daisy\", \"sunflowers\", \"roses\"]\n",
    "N_CLASSES = len(DATA_LABELS)\n",
    "\n",
    "data_paths = glob.glob(DATA_DIR + DATA_LABELS[0] + '/*.jpg', recursive=True)\n",
    "data_paths[:5]  # First 5 paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use pytorch's [Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) tools in order to create our own dataset. `__init__` will run once when the class is initialized. `__getitem__` returns our images and labels while randomly applying our `random_transforms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for l_idx, label in enumerate(DATA_LABELS):\n",
    "            data_paths = glob.glob(DATA_DIR + label + '/*.jpg', recursive=True)\n",
    "            for path in data_paths:\n",
    "                img = Image.open(path)\n",
    "                self.imgs.append(pre_transforms(img).to(device))\n",
    "                self.labels.append(l_idx)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = random_transforms(self.imgs[idx])\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "train_data = MyDataset()\n",
    "dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to initialize our U-Net. It's the same as before, but our `T` is much larger as are our `down_chs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 400\n",
    "B_start = 0.0001\n",
    "B_end = 0.02\n",
    "B = torch.linspace(B_start, B_end, T).to(device)\n",
    "ddpm = ddpm_utils.DDPM(B, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_flowers = UNet_utils.UNet(\n",
    "    T, IMG_CH, IMG_SIZE, down_chs=(256, 256, 512), t_embed_dim=8, c_embed_dim=N_CLASSES\n",
    ")\n",
    "print(\"Num params: \", sum(p.numel() for p in model_flowers.parameters()))\n",
    "model_flowers = torch.compile(model_flowers.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to sample our diffusion model, so we can generate images during and after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_flowers(n_classes):\n",
    "    c_test = torch.arange(n_classes).to(device)\n",
    "    c_hot_test, c_mask_test = get_context_mask(c_test, 0)\n",
    "    x_gen, x_gen_store = sample_w(model_flowers, INPUT_SIZE, T, c_hot_test)\n",
    "    return x_gen, x_gen_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Below is our new training loop. We've turned it into a function, but it's not complete yet. Each `FIXME` should be replaced with one of:\n",
    "* `dataloader`\n",
    "* `epochs`\n",
    "* `n_classes`\n",
    "* `c_drop_prob`\n",
    "\n",
    "If needed, click the `...` for a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_flowers(dataloader, epochs=100, n_classes=N_CLASSES, c_drop_prob=0.1, save_dir = \"04_images/\"):\n",
    "    lrate = 1e-4\n",
    "    optimizer = torch.optim.Adam(model_flowers.parameters(), lr=lrate)\n",
    "\n",
    "    model_flowers.train()\n",
    "    for epoch in range(FIXME):\n",
    "        for step, batch in enumerate(FIXME):\n",
    "            optimizer.zero_grad()\n",
    "            t = torch.randint(0, T, (BATCH_SIZE,), device=device).float()\n",
    "            x = batch[0].to(device)\n",
    "            c_hot, c_mask = get_context_mask(batch[1], FIXME)\n",
    "            loss = ddpm.get_loss(model_flowers, x, t, c_hot, c_mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch} | Step {step:03d} | Loss: {loss.item()}\")\n",
    "        if epoch % 5 == 0 or epoch == int(epochs - 1):\n",
    "            x_gen, x_gen_store = sample_flowers(FIXME)\n",
    "            grid = make_grid(x_gen.cpu(), nrow=n_classes)\n",
    "            save_image(grid, save_dir + f\"image_ep{epoch:02}.png\")\n",
    "            print(\"saved images in \" + save_dir + f\" for episode {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_flowers(dataloader, epochs=100, n_classes=N_CLASSES, c_drop_prob=0.1, save_dir = \"04_images/\"):\n",
    "    lrate = 1e-4\n",
    "    optimizer = torch.optim.Adam(model_flowers.parameters(), lr=lrate)\n",
    "\n",
    "    c = torch.arange(n_classes).to(device)\n",
    "    c_hot_test, c_mask_test = get_context_mask(c, 0)\n",
    "\n",
    "    model_flowers.train()\n",
    "    for epoch in range(epochs):\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            t = torch.randint(0, T, (BATCH_SIZE,), device=device).float()\n",
    "            x = batch[0].to(device)\n",
    "            c_hot, c_mask = get_context_mask(batch[1], c_drop_prob)\n",
    "            loss = ddpm.get_loss(model_flowers, x, t, c_hot, c_mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch} | Step {step:03d} | Loss: {loss.item()}\")\n",
    "        if epoch % 5 == 0 or epoch == int(epochs - 1):\n",
    "            x_gen, x_gen_store = sample_flowers(n_classes)\n",
    "            grid = make_grid(x_gen.cpu(), nrow=n_classes)\n",
    "            save_image(grid, save_dir + f\"image_ep{epoch:02}.png\")\n",
    "            print(\"saved images in \" + save_dir + f\" for episode {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moment of truth! Let's see how the diffusion model handles color images. The model will take about **fifteen minutes** to train. Once it starts training, grab a coffee, tea, or cozy beverage of choice. Then, check out the test outputs in the `04_images` directory. Flowers are recognizable at epoch 50, and the model will really hit its stride at epoch 100. It's fun to watch it learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flowers(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the random nature of diffusion models, some images will appear better than others. Try resampling until there is an output you'd like to keep as a souvenir. Then, run the cell afterwards to turn it into an animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x_gen, x_gen_store = sample_flowers(N_CLASSES)\n",
    "grid = make_grid(x_gen.cpu(), nrow=N_CLASSES)\n",
    "other_utils.show_tensor_image([grid])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [other_utils.to_image(make_grid(x_gen.cpu(), nrow=N_CLASSES)) for x_gen in x_gen_store]\n",
    "other_utils.save_animation(grids, \"04_images/flowers.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for a few minutes of training! In the next notebook, we'll make the results even better with a full text-to-image pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8hHZEaPIo0A"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
