{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73275a49",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60ab08",
   "metadata": {},
   "source": [
    "# 6. Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890fdc5d",
   "metadata": {},
   "source": [
    "Congratulations on going through today's course! Hope it was a fun journey with some new skills as souvenirs. Now it's time to put those skills to the test.\n",
    "\n",
    "In this assessment, the challenge is to train a new model that is able to generate handwritten images based on the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database). Traditionally, neural networks have a test dataset, but that is not necessarily the case with generative AI. Beauty is in the eye of the beholder, and it is up to you as the developer if overfitting is acceptable or not.\n",
    "\n",
    "So instead, we have created a classifier model that has been trained on the MNIST dataset. It has an accuracy on the MNIST test dataset of over 99%. If this model can correctly identify 95% of your generated images, you will earn a certificate!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1083571",
   "metadata": {},
   "source": [
    "## 6.1 The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb74e7",
   "metadata": {},
   "source": [
    "Let's get started, below are the libraries used in this assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c241ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "# User defined libraries\n",
    "from utils import other_utils\n",
    "from utils import ddpm_utils\n",
    "from utils import UNet_utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d69e9d3",
   "metadata": {},
   "source": [
    "The FashionMnist dataset we used earlier is structurally similar to MNIST, so we will use much of the same code to load it. We will not randomly flip horizontally because numbers are not typically meant to be read backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST(data_transform, train=True):\n",
    "    return torchvision.datasets.MNIST(\n",
    "        \"./data/\",\n",
    "        download=True,\n",
    "        train=train,\n",
    "        transform=data_transform,\n",
    "    )\n",
    "\n",
    "def load_transformed_MNIST(img_size, batch_size):\n",
    "    data_transforms = [\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),  # Scales data into [0,1]\n",
    "    ]\n",
    "\n",
    "    data_transform = transforms.Compose(data_transforms)\n",
    "    train_set = load_MNIST(data_transform, train=True)\n",
    "    test_set = load_MNIST(data_transform, train=False)\n",
    "    data = torch.utils.data.ConcatDataset([train_set, test_set])\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    return data, dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8313fe37",
   "metadata": {},
   "source": [
    "The classifier model we will be challenging expects the image size to be `28 by 28` pixels. The images are also in black and white. There are `10` classes, one for each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28\n",
    "IMG_CH = 1\n",
    "BATCH_SIZE = 128\n",
    "N_CLASSES = 10\n",
    "data, dataloader = load_transformed_MNIST(IMG_SIZE, BATCH_SIZE)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d3353",
   "metadata": {},
   "source": [
    "## 6.2 Setting Up Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541da32",
   "metadata": {},
   "source": [
    "Let's begin by setting up the diffusion process. In the interest of time, we have already listed the recommended hyperparameters for the `Beta` schedule below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 10\n",
    "ncols = 15\n",
    "\n",
    "T = nrows * ncols\n",
    "B_start = 0.0001\n",
    "B_end = 0.02\n",
    "B = torch.linspace(B_start, B_end, T).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4ad7f",
   "metadata": {},
   "source": [
    "**TODO**: We still need to math out some of the variables we'll be using in our `q` and `reverse_q` function.  Can you replace the `FIXME`s below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1.0 - B\n",
    "a_bar = FIXME(a, dim=0)\n",
    "sqrt_a_bar = FIXME(a_bar)  # Mean Coefficient\n",
    "sqrt_one_minus_a_bar = FIXME(1 - a_bar)  # St. Dev. Coefficient\n",
    "\n",
    "# Reverse diffusion variables\n",
    "sqrt_a_inv = FIXME(1 / a)\n",
    "pred_noise_coeff = (1 - a) / FIXME(1 - a_bar)  # Predicted Noise Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4247201",
   "metadata": {},
   "source": [
    "**TODO**: The `q` function below is almost done, but we need to find the right ratio of image to noise. How was that done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(x_0, t):\n",
    "        t = t.int()\n",
    "        noise = torch.randn_like(x_0)\n",
    "        sqrt_a_bar_t = sqrt_a_bar[t, None, None, None]\n",
    "        sqrt_one_minus_a_bar_t = sqrt_one_minus_a_bar[t, None, None, None]\n",
    "\n",
    "        x_t = FIXME * x_0 + FIXME * noise\n",
    "        return x_t, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83430633",
   "metadata": {},
   "source": [
    "Please take a moment to verify the results are what you would expect. Does the image start clearly identifiable and then becomes lost in the noise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "x_0 = data[0][0].to(device)\n",
    "xs = []\n",
    "\n",
    "for t in range(T):\n",
    "    t_tenser = torch.Tensor([t]).type(torch.int64)\n",
    "    x_t, _ = q(x_0, t_tenser)\n",
    "    img = torch.squeeze(x_t).cpu()\n",
    "    xs.append(img)\n",
    "    ax = plt.subplot(nrows, ncols, t + 1)\n",
    "    ax.axis('off')\n",
    "    other_utils.show_tensor_image(x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac62782",
   "metadata": {},
   "source": [
    "**TODO**: The `reverse_q` function is mostly complete, but there are a few `FIXME`s. Each `FIXME` can be one of:\n",
    "* `x_t` - the latent image\n",
    "* `t` - the current timestep\n",
    "* `e_t` - the predicted noise added at the current timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def reverse_q(x_t, t, e_t):\n",
    "    t = t.int()\n",
    "    pred_noise_coeff_t = pred_noise_coeff[t]\n",
    "    sqrt_a_inv_t = sqrt_a_inv[t]\n",
    "    u_t = sqrt_a_inv_t * (FIXME - pred_noise_coeff_t * FIXME)\n",
    "    if FIXME[0] == 0:  # All t values should be the same\n",
    "        return u_t  # Reverse diffusion complete!\n",
    "    else:\n",
    "        B_t = B[t - 1]  # Apply noise from the previos timestep\n",
    "        new_noise = torch.randn_like(x_t)\n",
    "        return u_t + torch.sqrt(B_t) * new_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7bf714",
   "metadata": {},
   "source": [
    "## 6.3 Setting up a U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674845f5",
   "metadata": {},
   "source": [
    "We will be using the same U-Net architecture that we have previously used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6427d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self, T, img_ch, img_size, down_chs=(64, 64, 128), t_embed_dim=8, c_embed_dim=10\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
    "        latent_image_size = img_size // 4  # 2 ** (len(down_chs) - 1)\n",
    "        small_group_size = 8\n",
    "        big_group_size = 32\n",
    "\n",
    "        # Inital convolution\n",
    "        self.down0 = ResidualConvBlock(img_ch, down_chs[0], small_group_size)\n",
    "\n",
    "        # Downsample\n",
    "        self.down1 = DownBlock(down_chs[0], down_chs[1], big_group_size)\n",
    "        self.down2 = DownBlock(down_chs[1], down_chs[2], big_group_size)\n",
    "        self.to_vec = nn.Sequential(nn.Flatten(), nn.GELU())\n",
    "\n",
    "        # Embeddings\n",
    "        self.dense_emb = nn.Sequential(\n",
    "            nn.Linear(down_chs[2] * latent_image_size**2, down_chs[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], down_chs[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(down_chs[1], down_chs[2] * latent_image_size**2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.sinusoidaltime = SinusoidalPositionEmbedBlock(t_embed_dim)\n",
    "        self.t_emb1 = EmbedBlock(t_embed_dim, up_chs[0])\n",
    "        self.t_emb2 = EmbedBlock(t_embed_dim, up_chs[1])\n",
    "        self.c_embed1 = EmbedBlock(c_embed_dim, up_chs[0])\n",
    "        self.c_embed2 = EmbedBlock(c_embed_dim, up_chs[1])\n",
    "\n",
    "        # Upsample\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.Unflatten(1, (up_chs[0], latent_image_size, latent_image_size)),\n",
    "            GELUConvBlock(up_chs[0], up_chs[0], big_group_size),\n",
    "        )\n",
    "        self.up1 = UpBlock(up_chs[0], up_chs[1], big_group_size)\n",
    "        self.up2 = UpBlock(up_chs[1], up_chs[2], big_group_size)\n",
    "\n",
    "        # Match output channels and one last concatenation\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * up_chs[-1], up_chs[-1], 3, 1, 1),\n",
    "            nn.GroupNorm(small_group_size, up_chs[-1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(up_chs[-1], img_ch, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c, c_mask):\n",
    "        down0 = self.down0(x)\n",
    "        down1 = self.down1(down0)\n",
    "        down2 = self.down2(down1)\n",
    "        latent_vec = self.to_vec(down2)\n",
    "\n",
    "        latent_vec = self.dense_emb(latent_vec)\n",
    "        t = t.float() / self.T  # Convert from [0, T] to [0, 1]\n",
    "        t = self.sinusoidaltime(t)\n",
    "        t_emb1 = self.t_emb1(t)\n",
    "        t_emb2 = self.t_emb2(t)\n",
    "\n",
    "        c = c * c_mask\n",
    "        c_emb1 = self.c_embed1(c)\n",
    "        c_emb2 = self.c_embed2(c)\n",
    "\n",
    "        up0 = self.up0(latent_vec)\n",
    "        up1 = self.up1(c_emb1 * up0 + t_emb1, down2)\n",
    "        up2 = self.up2(c_emb2 * up1 + t_emb2, down1)\n",
    "        return self.out(torch.cat((up2, down0), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81457189",
   "metadata": {},
   "source": [
    "**TODO**: Unfortunately, the names of the module blocks have been scrambled. Can you add the correct module name based on what the function is doing? There is one of each of:\n",
    "* `GELUConvBlock`\n",
    "* `RearrangePoolBlock`\n",
    "* `DownBlock`\n",
    "* `UpBlock`\n",
    "* `SinusoidalPositionEmbedBlock`\n",
    "* `EmbedBlock`\n",
    "* `ResidualConvBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIXME(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs, group_size):\n",
    "        super(DownBlock, self).__init__()\n",
    "        layers = [\n",
    "            GELUConvBlock(in_chs, out_chs, group_size),\n",
    "            GELUConvBlock(out_chs, out_chs, group_size),\n",
    "            RearrangePoolBlock(out_chs, group_size),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d1160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIXME(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedBlock, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            nn.Unflatten(1, (emb_dim, 1, 1)),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb3513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIXME(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, group_size):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_ch, out_ch, 3, 1, 1),\n",
    "            nn.GroupNorm(group_size, out_ch),\n",
    "            nn.GELU(),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIXME(nn.Module):\n",
    "    def __init__(self, in_chs, group_size):\n",
    "        super().__init__()\n",
    "        self.rearrange = Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2)\n",
    "        self.conv = GELUConvBlock(4 * in_chs, in_chs, group_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rearrange(x)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e16954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIXME(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs, group_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = GELUConvBlock(in_chs, out_chs, group_size)\n",
    "        self.conv2 = GELUConvBlock(out_chs, out_chs, group_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        out = x1 + x2\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIXME(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fdcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIXME(nn.Module):\n",
    "    def __init__(self, in_chs, out_chs, group_size):\n",
    "        super(UpBlock, self).__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(2 * in_chs, out_chs, 2, 2),\n",
    "            GELUConvBlock(out_chs, out_chs, group_size),\n",
    "            GELUConvBlock(out_chs, out_chs, group_size),\n",
    "            GELUConvBlock(out_chs, out_chs, group_size),\n",
    "            GELUConvBlock(out_chs, out_chs, group_size),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c053d8d",
   "metadata": {},
   "source": [
    "Now that all the pieces have been defined, let's define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f68053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet_utils.UNet(\n",
    "    T, IMG_CH, IMG_SIZE, down_chs=(64, 64, 128), t_embed_dim=8, c_embed_dim=N_CLASSES\n",
    ")\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "model = torch.compile(model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d2d0f",
   "metadata": {},
   "source": [
    "## 6.4 Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ee902",
   "metadata": {},
   "source": [
    "**TODO**: We should create a function to randomly drop the context. What was the function to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_mask(c, drop_prob):\n",
    "    c_hot = F.one_hot(c.to(torch.int64), num_classes=N_CLASSES).to(device)\n",
    "    c_mask = torch.FIXME(torch.ones_like(c_hot).float() - drop_prob).to(device)\n",
    "    return c_hot, c_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f820f1d",
   "metadata": {},
   "source": [
    "**TODO**: Next, let's define the loss function. What type of loss function should we be using?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, x_0, t, *model_args):\n",
    "    x_noisy, noise = q(x_0, t)\n",
    "    noise_pred = model(x_noisy, t/T, *model_args)\n",
    "    return F.FIXME(noise, noise_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb1a5b",
   "metadata": {},
   "source": [
    "This is mostly for our benefit to verify the model is training correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c345ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(model, img_ch, img_size, ncols, *model_args, axis_on=False):\n",
    "    # Noise to generate images from\n",
    "    x_t = torch.randn((1, img_ch, img_size, img_size), device=device)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    hidden_rows = T / ncols\n",
    "    plot_number = 1\n",
    "\n",
    "    # Go from T to 0 removing and adding noise until t = 0\n",
    "    for i in range(0, T)[::-1]:\n",
    "        t = torch.full((1,), i, device=device).float()\n",
    "        e_t = model(x_t, t, *model_args)  # Predicted noise\n",
    "        x_t = reverse_q(x_t, t, e_t)\n",
    "        if i % hidden_rows == 0:\n",
    "            ax = plt.subplot(1, ncols+1, plot_number)\n",
    "            if not axis_on:\n",
    "                ax.axis('off')\n",
    "            other_utils.show_tensor_image(x_t.detach().cpu())\n",
    "            plot_number += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e5610",
   "metadata": {},
   "source": [
    "**TODO**: Time to train the model! Can you fix the `FIXME`s, and get it going?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b5070",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 5\n",
    "preview_c = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        c_drop_prob = FIXME\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).float()\n",
    "        x = batch[0].to(device)\n",
    "        c_hot, c_mask = get_context_mask(FIXME, c_drop_prob)  # New\n",
    "        loss = get_loss(model, x, t, c_hot, c_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 1 == 0 and step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step:03d} | Loss: {loss.item()} | C: {preview_c}\")\n",
    "            c_drop_prob = 0 # Do not drop context for preview\n",
    "            c_hot, c_mask = get_context_mask(torch.Tensor([preview_c]), c_drop_prob)\n",
    "            sample_images(model, IMG_CH, IMG_SIZE, ncols, c_hot, c_mask)\n",
    "            preview_c = (preview_c + 1) % N_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784eb180",
   "metadata": {},
   "source": [
    "## 6.5 Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8bedd0",
   "metadata": {},
   "source": [
    "This is the last piece to the puzzle. We could compare the generator against the classifier, but as is, it would be extremely lucky to get over 95% accuracy. Let's use `Classifier-Free Diffusion Guidance to improve our chances.\n",
    "\n",
    "**TODO**: There is one `FIXME` in the `sample_w` formula below. Can you remember the formula to add weight to the diffusion process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b10158",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_w(model, c, w):\n",
    "    input_size = (IMG_CH, IMG_SIZE, IMG_SIZE)\n",
    "    n_samples = len(c)\n",
    "    w = torch.tensor([w]).float()\n",
    "    w = w[:, None, None, None].to(device)  # Make w broadcastable\n",
    "    x_t = torch.randn(n_samples, *input_size).to(device)\n",
    "\n",
    "    # One c for each w\n",
    "    c = c.repeat(len(w), 1)\n",
    "\n",
    "    # Double the batch\n",
    "    c = c.repeat(2, 1)\n",
    "\n",
    "    # Don't drop context at test time\n",
    "    c_mask = torch.ones_like(c).to(device)\n",
    "    c_mask[n_samples:] = 0.0\n",
    "\n",
    "    x_t_store = []\n",
    "    for i in range(0, T)[::-1]:\n",
    "        # Duplicate t for each sample\n",
    "        t = torch.tensor([i]).to(device)\n",
    "        t = t.repeat(n_samples, 1, 1, 1)\n",
    "\n",
    "        # Double the batch\n",
    "        x_t = x_t.repeat(2, 1, 1, 1)\n",
    "        t = t.repeat(2, 1, 1, 1)\n",
    "\n",
    "        # Find weighted noise\n",
    "        e_t = model(x_t, t, c, c_mask)\n",
    "        e_t_keep_c = e_t[:n_samples]\n",
    "        e_t_drop_c = e_t[n_samples:]\n",
    "        e_t = FIXME\n",
    "\n",
    "        # Deduplicate batch for reverse diffusion\n",
    "        x_t = x_t[:n_samples]\n",
    "        t = t[:n_samples]\n",
    "        x_t = reverse_q(x_t, t, e_t)\n",
    "\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003d964",
   "metadata": {},
   "source": [
    "**TODO**: Let's test it out. Try running the cell below a few times. Can you make it so that the numbers are consistently recognizable by changing `w`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3203fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "w = 0.0  # Change me\n",
    "c = torch.arange(N_CLASSES).to(device)\n",
    "c_drop_prob = 0 \n",
    "c_hot, c_mask = get_context_mask(c, c_drop_prob)\n",
    "\n",
    "x_0 = sample_w(model, c_hot, 1.0)\n",
    "other_utils.to_image(make_grid(x_0.cpu(), nrow=N_CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2342b8",
   "metadata": {},
   "source": [
    "This is important for the automated grader. Is the output shape `[10, 1, 28, 28]`? If yes, you're ready to test the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ed679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f7807",
   "metadata": {},
   "source": [
    "## 6.6 Run the Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b2c329",
   "metadata": {},
   "source": [
    "To assess your model run the following two cells.\n",
    "\n",
    "**NOTE:** `run_assessment` assumes your model is named `model` and your diffusion weight is called `w`. If for any reason you have modified these variable names, please update the names of the arguments passed to `run_assessment`. If your results are close, but not quite there, try changing `w` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d3633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_assessment import run_assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25834296",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_assessment(model, sample_w, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90006a68",
   "metadata": {},
   "source": [
    "## 6.7 Generate a Certificate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4557a",
   "metadata": {},
   "source": [
    "If you passed the assessment, please return to the course page (shown below) and click the \"ASSESS TASK\" button, which will generate your certificate for the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25fa579",
   "metadata": {},
   "source": [
    "<img src=\"./images/assess_task.png\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c0679",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
